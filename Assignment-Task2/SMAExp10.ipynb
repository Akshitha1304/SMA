{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNOEyiCQnel4fMzLuO9WHj5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZZ_5R3NUho7","executionInfo":{"status":"ok","timestamp":1738125009135,"user_tz":-330,"elapsed":15415,"user":{"displayName":"AKSHITHA BAVISETTI","userId":"15959263960425790353"}},"outputId":"2c00f315-fed4-4545-c4a2-23babe874980"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ”¹ **Topic 1:** python | automation\n","ðŸ”¹ **Topic 2:** python | automation\n","ðŸ”¹ **Topic 3:** automation | python\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["#10 exper\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","\n","# Download stopwords if not available\n","nltk.download(\"stopwords\")\n","stop_words = set(stopwords.words(\"english\"))\n","\n","# Load dataset\n","file_path = \"/content/10.csv\"\n","df = pd.read_csv(file_path)\n","\n","# Function to preprocess text\n","def preprocess_text(text):\n","    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n","    text = re.sub(r\"@\\w+\", \"\", text)  # Remove mentions\n","    text = re.sub(r\"#\\w+\", \"\", text)  # Remove hashtags\n","    text = re.sub(r\"RT\\s+\", \"\", text)  # Remove RT (retweets)\n","    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters\n","    text = text.lower()  # Convert to lowercase\n","    words = text.split()\n","    words = [word for word in words if word not in stop_words]  # Remove stopwords\n","    return \" \".join(words)\n","\n","# Apply preprocessing\n","df[\"clean_text\"] = df[\"text\"].astype(str).apply(preprocess_text)\n","\n","# Convert text to document-term matrix\n","vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")\n","dtm = vectorizer.fit_transform(df[\"clean_text\"])\n","\n","# Apply LDA\n","lda_model = LatentDirichletAllocation(n_components=3, random_state=42)\n","lda_model.fit(dtm)\n","\n","# Display top words in each topic\n","def display_topics(model, feature_names, num_words=5):\n","    for topic_idx, topic in enumerate(model.components_):\n","        print(f\"ðŸ”¹ **Topic {topic_idx + 1}:**\", \" | \".join([feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]]))\n","\n","# Show topics\n","feature_names = vectorizer.get_feature_names_out()\n","display_topics(lda_model, feature_names)"]}]}