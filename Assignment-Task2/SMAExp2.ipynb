{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dvB2Ce-2Nq3Q","executionInfo":{"status":"ok","timestamp":1738123213464,"user_tz":-330,"elapsed":403,"user":{"displayName":"AKSHITHA BAVISETTI","userId":"15959263960425790353"}},"outputId":"48f314d5-6452-425a-d2f1-49bc8b7aefe1"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                                post  \\\n","0  Wow! This is an awesome blog post: https://www...   \n","1  Feeling so tired today... Need coffee ‚òï #monda...   \n","2  Check out my new video on YouTube! üé• https://w...   \n","3  Can't believe this news! Read more: https://ww...   \n","4  Just had the best pasta ever! üçùüòç Highly recomm...   \n","\n","                                        cleaned_post  \n","0             wow awesome blog post blogging readnow  \n","1        feeling tired today need coffee mondayblues  \n","2                       check new video youtube vlog  \n","3                cant believe news read breakingnews  \n","4  best pasta ever highly recommend place foodie yum  \n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["#2) cleaning data using stopwords,urls and special characters from the social media posts\n","import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","df = pd.read_csv(\"/content/social_media_dataset.csv\")\n","def preprocess_text(text):\n","    \"\"\"\n","    Removes URLs, special characters, and stopwords from the given text.\n","\n","    Args:\n","        text (str): The original text.\n","\n","    Returns:\n","        str: The cleaned text.\n","    \"\"\"\n","    if pd.isnull(text):\n","        return \"\"\n","    text = re.sub(r'http\\S+', '', text)\n","    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n","    words = text.lower().split()\n","    stop_words = set(stopwords.words('english'))\n","    words = [word for word in words if word not in stop_words]\n","    return ' '.join(words)\n","\n","df['cleaned_post'] = df['post'].apply(preprocess_text)\n","\n","df.to_csv(\"cleaned_social_media_dataset.csv\", index=False)\n","\n","print(df.head())\n"]}]}